{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"alcohol_list.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, GHO (CODE): string, GHO (DISPLAY): string, GHO (URL): string, DATASOURCE (CODE): string, DATASOURCE (DISPLAY): string, DATASOURCE (URL): string, PUBLISHSTATE (CODE): string, PUBLISHSTATE (DISPLAY): string, PUBLISHSTATE (URL): string, YEAR (CODE): string, YEAR (DISPLAY): string, YEAR (URL): string, REGION (CODE): string, REGION (DISPLAY): string, REGION (URL): string, COUNTRY (CODE): string, COUNTRY (DISPLAY): string, COUNTRY (URL): string, ALCOHOLTYPE (CODE): string, ALCOHOLTYPE (DISPLAY): string, ALCOHOLTYPE (URL): string, Display Value: string, Numeric: string, Low: string, High: string, StdErr: string, StdDev: string, Comments: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+-----------------+--------------------+----------------+-------------------+----------------------+------------------+-----------+--------------+----------+-------------+--------------------+------------+--------------+--------------------+-------------+------------------+---------------------+-----------------+-------------+-------+----+----+------+------+--------+\n",
      "|   GHO (CODE)|       GHO (DISPLAY)|           GHO (URL)|DATASOURCE (CODE)|DATASOURCE (DISPLAY)|DATASOURCE (URL)|PUBLISHSTATE (CODE)|PUBLISHSTATE (DISPLAY)|PUBLISHSTATE (URL)|YEAR (CODE)|YEAR (DISPLAY)|YEAR (URL)|REGION (CODE)|    REGION (DISPLAY)|REGION (URL)|COUNTRY (CODE)|   COUNTRY (DISPLAY)|COUNTRY (URL)|ALCOHOLTYPE (CODE)|ALCOHOLTYPE (DISPLAY)|ALCOHOLTYPE (URL)|Display Value|Numeric| Low|High|StdErr|StdDev|Comments|\n",
      "+-------------+--------------------+--------------------+-----------------+--------------------+----------------+-------------------+----------------------+------------------+-----------+--------------+----------+-------------+--------------------+------------+--------------+--------------------+-------------+------------------+---------------------+-----------------+-------------+-------+----+----+------+------+--------+\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHAUS|         Data source|            null|          PUBLISHED|             Published|              null|       2011|          2011|      null|          WPR|     Western Pacific|        null|           AUS|           Australia|         null|           SA_WINE|                 Wine|             null|         3.79|   3.79|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHKAZ|         Data source|            null|          PUBLISHED|             Published|              null|       2006|          2006|      null|          EUR|              Europe|        null|           KAZ|          Kazakhstan|         null|           SA_BEER|                 Beer|             null|         2.22|   2.22|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHTZA|         Data source|            null|          PUBLISHED|             Published|              null|       2010|          2010|      null|          AFR|              Africa|        null|           TZA|United Republic o...|         null|          SA_TOTAL|            All types|             null|         4.19|   4.19|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHNZL|         Data source|            null|          PUBLISHED|             Published|              null|       2012|          2012|      null|          WPR|     Western Pacific|        null|           NZL|         New Zealand|         null|          SA_TOTAL|            All types|             null|         9.20|    9.2|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHDZA|         Data source|            null|          PUBLISHED|             Published|              null|       2005|          2005|      null|          AFR|              Africa|        null|           DZA|             Algeria|         null|        SA_SPIRITS|              Spirits|             null|         0.02|   0.02|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHCHE|         Data source|            null|          PUBLISHED|             Published|              null|       2003|          2003|      null|          EUR|              Europe|        null|           CHE|         Switzerland|         null|           SA_BEER|                 Beer|             null|         3.37|   3.37|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHCAF|         Data source|            null|          PUBLISHED|             Published|              null|       2009|          2009|      null|          AFR|              Africa|        null|           CAF|Central African R...|         null|           SA_WINE|                 Wine|             null|         0.01|   0.01|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHUKR|         Data source|            null|          PUBLISHED|             Published|              null|       2004|          2004|      null|          EUR|              Europe|        null|           UKR|             Ukraine|         null|          SA_TOTAL|            All types|             null|         6.79|   6.79|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHECU|         Data source|            null|          PUBLISHED|             Published|              null|       2011|          2011|      null|          AMR|            Americas|        null|           ECU|             Ecuador|         null|        SA_SPIRITS|              Spirits|             null|         1.15|   1.15|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHGIN|         Data source|            null|          PUBLISHED|             Published|              null|       2003|          2003|      null|          AFR|              Africa|        null|           GIN|              Guinea|         null|        SA_SPIRITS|              Spirits|             null|         0.00|    0.0|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHKNA|         Data source|            null|          PUBLISHED|             Published|              null|       2013|          2013|      null|          AMR|            Americas|        null|           KNA|Saint Kitts and N...|         null|           SA_BEER|                 Beer|             null|         3.96|   3.96|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHSOM|         Data source|            null|          PUBLISHED|             Published|              null|       2000|          2000|      null|          EMR|Eastern Mediterra...|        null|           SOM|             Somalia|         null|           SA_BEER|                 Beer|             null|         0.00|    0.0|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHEST|         Data source|            null|          PUBLISHED|             Published|              null|       2004|          2004|      null|          EUR|              Europe|        null|           EST|             Estonia|         null|  SA_OTHER_ALCOHOL| Other alcoholic b...|             null|         2.39|   2.39|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHKWT|         Data source|            null|          PUBLISHED|             Published|              null|       2004|          2004|      null|          EMR|Eastern Mediterra...|        null|           KWT|              Kuwait|         null|           SA_BEER|                 Beer|             null|         0.00|    0.0|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHMDA|         Data source|            null|          PUBLISHED|             Published|              null|       2007|          2007|      null|          EUR|              Europe|        null|           MDA| Republic of Moldova|         null|  SA_OTHER_ALCOHOL| Other alcoholic b...|             null|         0.41|   0.41|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHMDG|         Data source|            null|          PUBLISHED|             Published|              null|       2012|          2012|      null|          AFR|              Africa|        null|           MDG|          Madagascar|         null|  SA_OTHER_ALCOHOL| Other alcoholic b...|             null|         0.00|    0.0|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHGEO|         Data source|            null|          PUBLISHED|             Published|              null|       2010|          2010|      null|          EUR|              Europe|        null|           GEO|             Georgia|         null|        SA_SPIRITS|              Spirits|             null|         2.11|   2.11|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHRUS|         Data source|            null|          PUBLISHED|             Published|              null|       2011|          2011|      null|          EUR|              Europe|        null|           RUS|  Russian Federation|         null|           SA_WINE|                 Wine|             null|         1.24|   1.24|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHBGD|         Data source|            null|          PUBLISHED|             Published|              null|       2007|          2007|      null|         SEAR|     South-East Asia|        null|           BGD|          Bangladesh|         null|          SA_TOTAL|            All types|             null|         0.00|    0.0|null|null|  null|  null|    null|\n",
      "|SA_0000001400|Alcohol  recorded...|http://apps.who.i...|         GISAHCOK|         Data source|            null|          PUBLISHED|             Published|              null|       2000|          2000|      null|          WPR|     Western Pacific|        null|           COK|        Cook Islands|         null|  SA_OTHER_ALCOHOL| Other alcoholic b...|             null|         0.00|    0.0|null|null|  null|  null|    null|\n",
      "+-------------+--------------------+--------------------+-----------------+--------------------+----------------+-------------------+----------------------+------------------+-----------+--------------+----------+-------------+--------------------+------------+--------------+--------------------+-------------+------------------+---------------------+-----------------+-------------+-------+----+----+------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- GHO (CODE): string (nullable = true)\n",
      " |-- GHO (DISPLAY): string (nullable = true)\n",
      " |-- GHO (URL): string (nullable = true)\n",
      " |-- DATASOURCE (CODE): string (nullable = true)\n",
      " |-- DATASOURCE (DISPLAY): string (nullable = true)\n",
      " |-- DATASOURCE (URL): string (nullable = true)\n",
      " |-- PUBLISHSTATE (CODE): string (nullable = true)\n",
      " |-- PUBLISHSTATE (DISPLAY): string (nullable = true)\n",
      " |-- PUBLISHSTATE (URL): string (nullable = true)\n",
      " |-- YEAR (CODE): integer (nullable = true)\n",
      " |-- YEAR (DISPLAY): integer (nullable = true)\n",
      " |-- YEAR (URL): string (nullable = true)\n",
      " |-- REGION (CODE): string (nullable = true)\n",
      " |-- REGION (DISPLAY): string (nullable = true)\n",
      " |-- REGION (URL): string (nullable = true)\n",
      " |-- COUNTRY (CODE): string (nullable = true)\n",
      " |-- COUNTRY (DISPLAY): string (nullable = true)\n",
      " |-- COUNTRY (URL): string (nullable = true)\n",
      " |-- ALCOHOLTYPE (CODE): string (nullable = true)\n",
      " |-- ALCOHOLTYPE (DISPLAY): string (nullable = true)\n",
      " |-- ALCOHOLTYPE (URL): string (nullable = true)\n",
      " |-- Display Value: string (nullable = true)\n",
      " |-- Numeric: double (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- StdErr: string (nullable = true)\n",
      " |-- StdDev: string (nullable = true)\n",
      " |-- Comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13235"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_whatever = df.select('YEAR (DISPLAY)', 'REGION (DISPLAY)', 'COUNTRY (DISPLAY)', 'ALCOHOLTYPE (DISPLAY)', 'Display Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(YEAR (DISPLAY)=2011, REGION (DISPLAY)='Western Pacific', COUNTRY (DISPLAY)='Australia', ALCOHOLTYPE (DISPLAY)='Wine', Display Value='3.79')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_whatever.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|    Display Value|\n",
      "+-------+-----------------+\n",
      "|  count|            13235|\n",
      "|   mean|2.022251300887666|\n",
      "| stddev|2.792009361570929|\n",
      "|    min|                0|\n",
      "|    max|          No data|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_whatever.describe('Display Value').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-----------------+---------------------+-------------+\n",
      "|YEAR (DISPLAY)|REGION (DISPLAY)|COUNTRY (DISPLAY)|ALCOHOLTYPE (DISPLAY)|Display Value|\n",
      "+--------------+----------------+-----------------+---------------------+-------------+\n",
      "|          2014|          Europe|          Ireland|                 Beer|         5.13|\n",
      "|          2014|          Europe|          Ireland|            All types|        10.75|\n",
      "|          2014|          Europe|          Ireland|              Spirits|         1.97|\n",
      "|          2014|          Europe|          Ireland| Other alcoholic b...|         0.85|\n",
      "|          2014|          Europe|          Ireland|                 Wine|         2.80|\n",
      "+--------------+----------------+-----------------+---------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_whatever.filter((df['COUNTRY (DISPLAY)']=='Ireland') & (df['YEAR (DISPLAY)']==2014)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-----------------+---------------------+-------------+\n",
      "|YEAR (DISPLAY)|REGION (DISPLAY)|COUNTRY (DISPLAY)|ALCOHOLTYPE (DISPLAY)|Display Value|\n",
      "+--------------+----------------+-----------------+---------------------+-------------+\n",
      "|          2014|          Europe|          Ireland|                 Beer|         5.13|\n",
      "|          2014|          Europe|          Ireland|            All types|        10.75|\n",
      "|          2014|          Europe|          Ireland|              Spirits|         1.97|\n",
      "|          2014|          Europe|          Ireland| Other alcoholic b...|         0.85|\n",
      "|          2014|          Europe|          Ireland|                 Wine|         2.80|\n",
      "+--------------+----------------+-----------------+---------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_whatever.where((df['COUNTRY (DISPLAY)']=='Ireland') & (df['YEAR (DISPLAY)']==2014)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_whatever = df_whatever.withColumnRenamed(\"YEAR (DISPLAY)\", \"year\").withColumnRenamed(\"REGION (DISPLAY)\", \"region\").withColumnRenamed(\"COUNTRY (DISPLAY)\", \"country\").withColumnRenamed(\"ALCOHOLTYPE (DISPLAY)\", \"alc_type\").withColumnRenamed(\"Display Value\", \"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_whatever.registerTempTable('df_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+-----+\n",
      "|year|              region|             country|            alc_type|value|\n",
      "+----+--------------------+--------------------+--------------------+-----+\n",
      "|2011|     Western Pacific|           Australia|                Wine| 3.79|\n",
      "|2006|              Europe|          Kazakhstan|                Beer| 2.22|\n",
      "|2010|              Africa|United Republic o...|           All types| 4.19|\n",
      "|2012|     Western Pacific|         New Zealand|           All types| 9.20|\n",
      "|2005|              Africa|             Algeria|             Spirits| 0.02|\n",
      "|2003|              Europe|         Switzerland|                Beer| 3.37|\n",
      "|2009|              Africa|Central African R...|                Wine| 0.01|\n",
      "|2004|              Europe|             Ukraine|           All types| 6.79|\n",
      "|2011|            Americas|             Ecuador|             Spirits| 1.15|\n",
      "|2003|              Africa|              Guinea|             Spirits| 0.00|\n",
      "|2013|            Americas|Saint Kitts and N...|                Beer| 3.96|\n",
      "|2000|Eastern Mediterra...|             Somalia|                Beer| 0.00|\n",
      "|2004|              Europe|             Estonia|Other alcoholic b...| 2.39|\n",
      "|2004|Eastern Mediterra...|              Kuwait|                Beer| 0.00|\n",
      "|2007|              Europe| Republic of Moldova|Other alcoholic b...| 0.41|\n",
      "|2012|              Africa|          Madagascar|Other alcoholic b...| 0.00|\n",
      "|2010|              Europe|             Georgia|             Spirits| 2.11|\n",
      "|2011|              Europe|  Russian Federation|                Wine| 1.24|\n",
      "|2007|     South-East Asia|          Bangladesh|           All types| 0.00|\n",
      "|2000|     Western Pacific|        Cook Islands|Other alcoholic b...| 0.00|\n",
      "|2010|              Africa|          Mozambique|             Spirits| 0.19|\n",
      "|2009|            Americas|                Cuba|Other alcoholic b...| 0.02|\n",
      "|2013|              Europe|            Bulgaria|           All types|12.06|\n",
      "|2000|              Europe|             Czechia|Other alcoholic b...| 0.00|\n",
      "|2002|              Africa|            Zimbabwe|                Wine| 0.28|\n",
      "|2001|              Europe|          Azerbaijan|                Wine| 0.05|\n",
      "|2000|     Western Pacific|               Japan|                Beer| 2.50|\n",
      "|2005|              Europe|             Romania|                Wine| 2.33|\n",
      "|2001|     Western Pacific|            Mongolia|             Spirits| 2.29|\n",
      "|2009|     Western Pacific|Micronesia (Feder...|           All types| 2.06|\n",
      "|2000|              Europe|             Iceland|                Beer| 3.03|\n",
      "|2007|              Africa|Democratic Republ...|                Beer| 0.49|\n",
      "|2011|              Europe|           Lithuania|Other alcoholic b...| 1.97|\n",
      "|2002|     Western Pacific|               Samoa|                Wine| 0.05|\n",
      "|2004|              Europe|             Finland|             Spirits| 2.79|\n",
      "|2007|     South-East Asia|               India|           All types| 1.59|\n",
      "|2004|Eastern Mediterra...|Syrian Arab Republic|             Spirits| 0.56|\n",
      "|2004|     Western Pacific|                Niue|           All types| 8.71|\n",
      "|2005|            Americas|            Barbados|                Wine| 0.75|\n",
      "|2002|            Americas|Saint Vincent and...|           All types| 4.94|\n",
      "|2008|Eastern Mediterra...|         Afghanistan|             Spirits| 0.02|\n",
      "|2002|              Europe|             Belgium|Other alcoholic b...| 0.00|\n",
      "|2013|     Western Pacific|         Philippines|           All types| 4.61|\n",
      "|2000|              Africa|             Namibia|             Spirits| 0.80|\n",
      "|2009|     Western Pacific|           Singapore|Other alcoholic b...| 0.03|\n",
      "|2003|            Americas|  Dominican Republic|Other alcoholic b...| 0.00|\n",
      "|2005|              Africa|Sao Tome and Prin...|             Spirits| 0.74|\n",
      "|2010|              Europe|             Andorra|                Wine| 4.39|\n",
      "|2003|            Americas|              Brazil|           All types| 6.95|\n",
      "|2000|     South-East Asia|               Nepal|             Spirits| 0.01|\n",
      "+----+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select * from df_').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|min(year)|max(year)|\n",
      "+---------+---------+\n",
      "|     2000|     2015|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select min(year), max(year) from df_').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|min(year)|max(year)|\n",
      "+---------+---------+\n",
      "|     2000|     2014|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select min(year), max(year) from df_ where country=\\'Ireland\\'').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----------+---------+-------+\n",
      "|year|region|   country| alc_type|  value|\n",
      "+----+------+----------+---------+-------+\n",
      "|2009|Europe|San Marino|All types|No data|\n",
      "|2009|Europe|    Monaco|All types|No data|\n",
      "|2010|Europe|    Monaco|All types|No data|\n",
      "|2000|Europe|    Monaco|All types|No data|\n",
      "|2006|Europe|San Marino|All types|No data|\n",
      "|2002|Europe|San Marino|All types|No data|\n",
      "|2003|Europe|San Marino|All types|No data|\n",
      "|2010|Europe|San Marino|All types|No data|\n",
      "|2000|Europe|San Marino|All types|No data|\n",
      "|2006|Europe|    Monaco|All types|No data|\n",
      "|2003|Europe|    Monaco|All types|No data|\n",
      "|2004|Europe|San Marino|All types|No data|\n",
      "|2007|Europe|San Marino|All types|No data|\n",
      "|2005|Europe|San Marino|All types|No data|\n",
      "|2004|Europe|    Monaco|All types|No data|\n",
      "|2007|Europe|    Monaco|All types|No data|\n",
      "|2001|Europe|San Marino|All types|No data|\n",
      "|2005|Europe|    Monaco|All types|No data|\n",
      "|2001|Europe|    Monaco|All types|No data|\n",
      "|2005|Europe|Montenegro|All types|No data|\n",
      "+----+------+----------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select * from df_ where region=\\'Europe\\' and alc_type=\\'All types\\' order by value desc').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\"\\nOperation not allowed: delete from(line 1, pos 0)\\n\\n== SQL ==\\ndelete from df_ where value='No data' \\n^^^\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/BigData/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/BigData/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o25.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nOperation not allowed: delete from(line 1, pos 0)\n\n== SQL ==\ndelete from df_ where value='No data' \n^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParserUtils$.operationNotAllowed(ParserUtils.scala:41)\n\tat org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitFailNativeCommand$1.apply(SparkSqlParser.scala:1047)\n\tat org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitFailNativeCommand$1.apply(SparkSqlParser.scala:1038)\n\tat org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:108)\n\tat org.apache.spark.sql.execution.SparkSqlAstBuilder.visitFailNativeCommand(SparkSqlParser.scala:1038)\n\tat org.apache.spark.sql.execution.SparkSqlAstBuilder.visitFailNativeCommand(SparkSqlParser.scala:55)\n\tat org.apache.spark.sql.catalyst.parser.SqlBaseParser$FailNativeCommandContext.accept(SqlBaseParser.java:782)\n\tat org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:18)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply(AstBuilder.scala:72)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply(AstBuilder.scala:72)\n\tat org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:108)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleStatement(AstBuilder.scala:71)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:70)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:69)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:100)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:69)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f4c420c60258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delete from df_ where value=\\'No data\\' \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/BigData/lib/python3.6/site-packages/pyspark/sql/context.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \"\"\"\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/BigData/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/BigData/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/BigData/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \"\\nOperation not allowed: delete from(line 1, pos 0)\\n\\n== SQL ==\\ndelete from df_ where value='No data' \\n^^^\\n\""
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"delete from df_ where value=\\'No data\\' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------------------+---------+-----+\n",
      "|year|region|             country| alc_type|value|\n",
      "+----+------+--------------------+---------+-----+\n",
      "|2014|Europe| Republic of Moldova|All types| 9.99|\n",
      "|2014|Europe|            Portugal|All types| 9.88|\n",
      "|2014|Europe|             Denmark|All types| 9.64|\n",
      "|2014|Europe|         Switzerland|All types| 9.61|\n",
      "|2014|Europe|              Serbia|All types| 9.09|\n",
      "|2014|Europe|             Finland|All types| 8.80|\n",
      "|2014|Europe|               Malta|All types| 8.49|\n",
      "|2014|Europe|             Ukraine|All types| 8.06|\n",
      "|2014|Europe|               Italy|All types| 7.56|\n",
      "|2014|Europe|              Greece|All types| 7.53|\n",
      "|2014|Europe|             Iceland|All types| 7.45|\n",
      "|2014|Europe|              Sweden|All types| 7.30|\n",
      "|2014|Europe|          Kazakhstan|All types| 6.29|\n",
      "|2014|Europe|             Georgia|All types| 6.13|\n",
      "|2014|Europe|              Norway|All types| 6.06|\n",
      "|2014|Europe|             Albania|All types| 4.51|\n",
      "|2014|Europe|Bosnia and Herzeg...|All types| 4.03|\n",
      "|2014|Europe|             Armenia|All types| 3.91|\n",
      "|2014|Europe|        Turkmenistan|All types| 2.90|\n",
      "|2014|Europe|              Israel|All types| 2.62|\n",
      "+----+------+--------------------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select * from df_ where region=\\'Europe\\' and year=2014 and alc_type=\\'All types\\' and value!=\\'No data\\' order by value desc').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelized Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelized collections are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. For example, here is how to create a parallelized collection holding the numbers 1 to 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, the distributed dataset (distData) can be operated on in parallel. For example, we can call distData.reduce(lambda a, b: a + b) to add up the elements of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n",
    "\n",
    "Text file RDDs can be created using SparkContext’s textFile method. This method takes an URI for the file (either a local path on the machine, or a hdfs://, s3a://, etc URI) and reads it as a collection of lines. Here is an example invocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"hdfs://nn1home:8020/input/war-and-peace.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "distFile = sc.textFile(\"data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, distFile can be acted on by dataset operations. For example, we can add up the sizes of all the lines using the map and reduce operations as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/barrysheppard/Github/DBSDataAnalysis/Tools/Spark/data.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e30d245df7f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdistFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/BigData/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/BigData/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/BigData/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/BigData/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/BigData/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/barrysheppard/Github/DBSDataAnalysis/Tools/Spark/data.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line defines a base RDD from an external file. This dataset is not loaded in memory or otherwise acted on: lines is merely a pointer to the file.   \n",
    "The second line defines lineLengths as the result of a map transformation. Again, lineLengths is not immediately computed, due to laziness.   \n",
    "Finally, we run reduce, which is an action. At this point Spark breaks the computation into tasks to run on separate machines, and each machine runs both its part of the map and a local reduction, returning only its answer to the driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"alcohol_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineLengths = rdd.map(lambda s: len(s))\n",
    "lineLengths.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalLength = lineLengths.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4621585"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
